{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05a5461f-9c25-4d2b-bdfd-0025a465593d",
   "metadata": {},
   "source": [
    "### Assignment No. :- 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4f684b-e7b7-4b30-b2a4-982d394507ad",
   "metadata": {},
   "source": [
    "#### Aim :- Text Preprocessing - Implement text preprocessing techniques, including tokenization and normalization, and apply it to any text data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668d177f-764e-4918-8f39-72e592edd9cd",
   "metadata": {},
   "source": [
    "##### Name :- Tamboli Majid Samir\n",
    "##### Class :- T.Y.A.I.-B\n",
    "##### Batch :- B\n",
    "##### Roll No. :- 2317126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04dd98c6-80d7-4967-9676-019ca6ee1d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk    ## Used for text processing tasks like tokenization, stemming, lemmatization\n",
    "import re      ## Used to clean text (remove symbols, numbers, punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bbf497a-a09f-449f-80ec-2984804f29cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading punkt_tab: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b50a87f-ca40-471e-8553-185b73fc8f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = \"Natural Language Processing is a branch of Artificial Intelligence. NLP is widely used in applications like chatbots, sentiment analysis, and machine translation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5f3b942-19a3-4602-b2bb-6e5325830e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercase Text is : \n",
      " natural language processing is a branch of artificial intelligence. nlp is widely used in applications like chatbots, sentiment analysis, and machine translation.\n"
     ]
    }
   ],
   "source": [
    "text_data = text_data.lower()       # To Convert The Text into LowerCase\n",
    "print(\"Lowercase Text is : \\n\",text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "747bf48b-0e12-4318-851f-dafb752f4057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Without Special Chracters : \n",
      " natural language processing is a branch of artificial intelligence nlp is widely used in applications like chatbots sentiment analysis and machine translation\n"
     ]
    }
   ],
   "source": [
    "text_data = re.sub(r'[^a-zA-Z0-9\\s]', '', text_data)        #  To Remove the Special Charcters From the text\n",
    "print(\"Text Without Special Chracters : \\n\", text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17b545cb-efe7-4e6d-8e0b-d52efb9cb7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Without Extra Spaces : \n",
      " natural language processing is a branch of artificial intelligence nlp is widely used in applications like chatbots sentiment analysis and machine translation\n"
     ]
    }
   ],
   "source": [
    "text_data = re.sub(r'\\s+', ' ', text_data).strip()         # To Remove the Extra Spaces\n",
    "print(\"Text Without Extra Spaces : \\n\", text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adf23b6-7417-4e27-ac4e-9be46cd750ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12753054-500f-4e02-bb76-0acc35e88fb1",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e5ba31c-048b-4987-906f-92cc37cfb5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens : \n",
      " ['natural', 'language', 'processing', 'is', 'a', 'branch', 'of', 'artificial', 'intelligence', 'nlp', 'is', 'widely', 'used', 'in', 'applications', 'like', 'chatbots', 'sentiment', 'analysis', 'and', 'machine', 'translation']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize            ## To Convert text data into individual words\n",
    "tokens = word_tokenize(text_data)\n",
    "print(f\"Tokens : \\n {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b27dd2-1c2f-4235-9e65-3252f043dd88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8793c133-a80f-4f68-88fb-d6c561634f93",
   "metadata": {},
   "source": [
    "#### Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebedb6a8-b496-40fd-995c-b13c666cbd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', 'branch', 'artificial', 'intelligence', 'nlp', 'widely', 'used', 'applications', 'like', 'chatbots', 'sentiment', 'analysis', 'machine', 'translation']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords                 # To Remove The Stopwords Like is,the,and\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [token for token in tokens if token not in stop_words]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12286639-39c2-411a-bbd6-ad1094a01497",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "878db013-1fc2-46ce-8a90-7edc0eacc286",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f721f127-c149-4088-8379-b9d9c9b324b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer    ## Stemming reduces words to their root form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7e5d7bc-0772-4939-9480-f4a02ee1e3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natur', 'languag', 'process', 'branch', 'artifici', 'intellig', 'nlp', 'wide', 'use', 'applic', 'like', 'chatbot', 'sentiment', 'analysi', 'machin', 'translat']\n"
     ]
    }
   ],
   "source": [
    "PR_stemmer = PorterStemmer()                     \n",
    "tokens_stem = [PR_stemmer.stem(token)\n",
    "          for token in tokens]\n",
    "print(tokens_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbda566f-c516-496e-810d-e4a8b1fe06a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natur', 'languag', 'process', 'branch', 'artifici', 'intellig', 'nlp', 'wide', 'use', 'applic', 'like', 'chatbot', 'sentiment', 'analysi', 'machin', 'translat']\n"
     ]
    }
   ],
   "source": [
    "SB_stemmer = SnowballStemmer('english')\n",
    "tokens_stem = [SB_stemmer.stem(token)\n",
    "          for token in tokens]\n",
    "print(tokens_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0631ac62-e686-42e2-bde2-ca2d5c0b26f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', 'branch', 'artificial', 'intelligence', 'nlp', 'widely', 'used', 'application', 'like', 'chatbots', 'sentiment', 'analysis', 'machine', 'translation']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "tokens_lem = [lemmatizer.lemmatize(token)\n",
    "          for token in tokens]\n",
    "print(tokens_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed118e1-0a14-4507-8386-3053762b496d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
